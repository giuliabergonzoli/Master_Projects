{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n!pip install attention\nfrom attention import Attention\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)\n\n\n#random seed\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the dataset\ndataset = pd.read_csv('../input/d/rossanalocatelli/training/Training.csv')\nprint(dataset.shape)\n\n#To convert into float32\ndataset = dataset.astype('float32')\n\ndataset.head()\ndataset.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_raw = dataset.iloc[:]\nprint(X_train_raw.shape)\n\n# Normalize both features and labels\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\n\nX_train_raw = (X_train_raw-X_min)/(X_max-X_min)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the first label 'Sponginess'**\n","metadata":{}},{"cell_type":"code","source":"#Deciding window and stride \nwindow = 200\nstride = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_sequences(df, target_labels=['Sponginess'], window=window, stride=stride, telescope=864): \n    assert window % stride == 0\n    dataset = []\n    labels = []\n    temp_df = df.copy().values\n    temp_label = df[target_labels].copy().values\n    padding_len = len(df)%window\n\n    if(padding_len != 0):\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float32')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float32')\n        temp_label = np.concatenate((padding,temp_label))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n        dataset.append(temp_df[idx:idx+window])\n        labels.append(temp_label[idx+window:idx+window+telescope])\n\n    dataset = np.array(dataset)\n    labels = np.array(labels)\n    return dataset, labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 500","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    \n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer) \n    convlstm = tfkl.LSTM(128, return_sequences=True,  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.Conv1D(256, 3, padding='same', activation='relu',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = tfkl.GRU(256, return_sequences=True,  kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(256, 3, padding='same', activation='relu',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm) \n    added = tf.keras.layers.Add()([x1, x2]) #skip connection\n    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5, seed=seed)(convlstm)\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-5), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_0')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the second label 'Wonder Level'**","metadata":{}},{"cell_type":"code","source":"window = 200\nstride = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0) \nfuture.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 500\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer) \n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.Conv1D(256, 3, padding='same', activation='relu',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = tfkl.GRU(256, return_sequences=True,  kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    added = tf.keras.layers.Add()([x1, x2])\n    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5, seed=seed)(convlstm)\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same', kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_1')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the third and seventh label 'Crunchiness' and 'Hype Root'**","metadata":{}},{"cell_type":"code","source":"window = 200\nstride = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0) \nfuture.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 500","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer)\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = tfkl.GRU(256, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    added = tf.keras.layers.Add()([x1, x2])\n    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5, seed=seed)(convlstm)\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same', kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_6')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the fourth 'Loudness on impact'**","metadata":{}},{"cell_type":"code","source":"window = 200\nstride = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 500","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer)\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.MaxPool1D(pool_size=4)(x1)\n    convlstm = tfkl.GRU(256, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    added = tf.keras.layers.Add()([x1, x2])\n    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5, seed=seed)(convlstm)\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same', kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_3')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the fifth label 'Meme creativity'**","metadata":{}},{"cell_type":"code","source":"window = 200\nstride = 10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 400","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    convlstm = tfkl.LSTM(64, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer)\n    x1 = tfkl.Conv1D(128, 3, padding='same', activation='relu',  kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = tfkl.GRU(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(128, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    added = tf.keras.layers.Add()([x1, x2])\n    convlstm = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.Dropout(.3, seed=seed)(convlstm)\n    convlstm = tfkl.LSTM(256, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = Attention()(convlstm)\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same', kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_4')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for the sixth label 'Soap slipperiness'**","metadata":{}},{"cell_type":"code","source":"window = 150\nstride = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = dataset[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_labels = dataset.columns\ntelescope = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 500","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(input_layer)\n    convlstm = tfkl.LSTM(128, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    x1 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    convlstm = tfkl.GRU(256, return_sequences=True, kernel_initializer = tfk.initializers.GlorotUniform(seed))(x1)\n    x2 = tfkl.Conv1D(256, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    added = tf.keras.layers.Add()([x1, x2])\n    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(added)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5, seed=seed)(convlstm)\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu', kernel_initializer = tfk.initializers.GlorotUniform(seed))(convlstm)\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same', kernel_initializer = tfk.initializers.GlorotUniform(seed))(output_layer)\n\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(1e-4), metrics=['mae'])\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.1,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n).history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('SubmissionModel_5')","metadata":{},"execution_count":null,"outputs":[]}]}